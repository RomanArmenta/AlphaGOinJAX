{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive dynamics of Ising spins in one dimension leveraging Reinforcement Learning\n",
    "\n",
    "In this work, they focus on the use of RL in active matter systems. Active matter systems are also many-particle systems, consisting of a large number of entities that take up energy from the environment and convert it into drected motion. It is inherently far from equilibrium. One of the interesting characteristics of these systems is the collective coherent motion along a common direction, also called flocking.\n",
    "\n",
    "### Model\n",
    "\n",
    "We start with $N$ active Ising spins ($s = \\pm 1$) that are randomly distributed along a length $L$ with periodic boundary conditions. To incorporate the RL approach, we define the state and action for each $i^{th}$ spin. The state of each $i^{th}$ spin is determined based on the direction of the spin with respect to its neighbours within the range $[x_i - \\delta x, x_i + \\delta x]$, where $x_i$ is the position of $i^{th}$ spin and $\\delta x$ is the interaction range and chosen as the unit of length in the system. For each $i^{th}$ spin at any instantaneous time $t$, we have two states: $S_i (t) = \\pm 1$. If the spin has the same direction as the majority of spins in the range, then its state $S_i (t) = +1$ and $S_i (t) = -1$ for the opposite direction. Further, the action $a_i (t)$ for each spin is either to flip or keep its orientation.\n",
    "\n",
    "Position update for the spin at each time step $\\Delta t$ is according to:\n",
    "\n",
    "$$x_i (t + \\Delta t) = x_i (t) + \\tilde{v}_i (t)s_i(t + \\Delta t)\\Delta t$$\n",
    "\n",
    "here, $x_i (t)$ and $\\tilde{v}_i (t)$ are the position and instantaneous self-propulsion speed of the $i^{th}$ spin at time $t$, respectively, and $s_i (t + \\Delta t)$ is the updated spin orientation of the $i^{th}$ spin, and $\\tilde{v}_i (t)$ is taken from a uniform distribution with nonzero positive lower and upper bounds $v_1$ and $v_2$, respectively. At every time step, the magnitude of $\\tilde{v}_i (t)$ is chosen randomly; hence, at every instance, each particle can take a random step size obtained from the distribution.\n",
    "\n",
    "The measure of learning is adopted by maintaining the cohesion among the spins. So, the spin receives feedback when it moves to a new position. The spin pays a cost if it loses the number of neighbours around it. Hence, the cost function $C_i (t + \\Delta t)$ for each spin is defined as:\n",
    "\n",
    "$$C_i (t + \\Delta t) = \\begin{cases}1, \\text{ if }n_i(t + \\Delta t)<n_i (t) \\\\ 0, \\text{ otherwise}\\end{cases}$$\n",
    "\n",
    "Here, $n_i(t)$ number of spins within the range $[x_i - \\delta x, x_i + \\delta x]$ at time $t$. The $Q$ matrix is updated at every time step, and initialize it at zeros. At each time, the matrix is updated as:\n",
    "\n",
    "$$Q_i [S_i(t), a_i(t)] \\leftarrow Q_i [S_i(t), a_i (t)](1-\\alpha) + \\alpha C_i(t + \\Delta t)$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. \n",
    "\n",
    "Further, the action is chosen based on $\\epsilon$-greedy algorithm:\n",
    "\n",
    "$$a_i(t) = \\begin{cases}\\text{random action, with probability } \\epsilon \\\\ \\text{argmin}Q_i [S_i(t), a_i(t)],\\text{ with probability }(1-\\epsilon)\\end{cases}$$\n",
    "\n",
    "We choose argmin for the update because the minimum $Q$ value tells that the cost value is also minimum for the corresponding state-action pair. \n",
    "\n",
    "To characterize the ordering in the system, we define average magnetization $\\braket{m}$ as the order parameter,\n",
    "\n",
    "$$\\braket{m} = \\frac{1}{N}\\sum_i s_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = 10  # Number of spins\n",
    "L = 10.0  # Length of the system\n",
    "delta_x = 1.0  # Interaction range\n",
    "v1, v2 = 0.1, 1.0  # Speed bounds\n",
    "alpha = 0.1  # Learning rate\n",
    "epsilon = 0.1  # Exploration rate\n",
    "delta_t = 0.1  # Time step\n",
    "num_actions = 2  # Number of actions (flip or keep)\n",
    "\n",
    "# Initialize positions, spins, and velocities\n",
    "key = jax.random.PRNGKey(0)\n",
    "positions = jax.random.uniform(key, (N,), minval=0, maxval=L)\n",
    "spins = jax.random.choice(key, jnp.array([-1, 1]), (N,))\n",
    "velocities = jax.random.uniform(key, (N,), minval=v1, maxval=v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=num_actions)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-network\n",
    "def create_train_state(key, learning_rate):\n",
    "    q_net = QNetwork()\n",
    "    params = q_net.init(key, jnp.ones((1, 2)))  # Input shape: (batch_size, num_features)\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=q_net.apply, params=params, tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "state = create_train_state(subkey, alpha)\n",
    "\n",
    "def get_neighbors(i, positions, spins):\n",
    "    left = positions[i] - delta_x\n",
    "    right = positions[i] + delta_x\n",
    "    mask = (positions >= left) & (positions <= right)\n",
    "    return spins[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_spin(i, spins, Q, epsilon):\n",
    "    if jax.random.uniform(key) < epsilon:\n",
    "        return -spins[i]  # Random action: flip the spin\n",
    "    else:\n",
    "        state = jnp.array([spins[i], jnp.sign(jnp.sum(get_neighbors(i, positions, spins)))])\n",
    "        q_values = Q.apply_fn(Q.params, state)\n",
    "        action = jnp.argmin(q_values)\n",
    "        return spins[i] if action == 0 else -spins[i]  # Greedy action based on Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_position(i, positions, spins, velocities):\n",
    "    return positions[i] + velocities[i] * spins[i] * delta_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(i, positions, spins):\n",
    "    neighbors_before = get_neighbors(i, positions, spins)\n",
    "    new_spin = update_spin(i, spins, state, epsilon)\n",
    "    new_position = update_position(i, positions, spins, velocities)\n",
    "    neighbors_after = get_neighbors(i, positions.at[i].set(new_position), spins.at[i].set(new_spin))\n",
    "    return 1 if len(neighbors_after) < len(neighbors_before) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(i, state, cost):\n",
    "    state_input = jnp.array([spins[i], jnp.sign(jnp.sum(get_neighbors(i, positions, spins)))])\n",
    "    q_values = state.apply_fn(state.params, state_input)\n",
    "    action = jnp.argmin(q_values)\n",
    "    target = q_values.at[action].set(cost)\n",
    "    loss = jnp.mean((q_values - target) ** 2)\n",
    "    grads = jax.grad(lambda params: jnp.mean((state.apply_fn(params, state_input) - target) ** 2))(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voy en:  0\n",
      "Voy en:  1\n",
      "Voy en:  2\n",
      "Voy en:  3\n",
      "Voy en:  4\n",
      "Voy en:  5\n",
      "Voy en:  6\n",
      "Voy en:  7\n",
      "Voy en:  8\n",
      "Voy en:  9\n",
      "Voy en:  10\n",
      "Voy en:  11\n",
      "Voy en:  12\n",
      "Voy en:  13\n",
      "Voy en:  14\n",
      "Voy en:  15\n",
      "Voy en:  16\n",
      "Voy en:  17\n",
      "Voy en:  18\n",
      "Voy en:  19\n",
      "Voy en:  20\n",
      "Voy en:  21\n",
      "Voy en:  22\n",
      "Voy en:  23\n",
      "Voy en:  24\n",
      "Voy en:  25\n",
      "Voy en:  26\n",
      "Voy en:  27\n",
      "Voy en:  28\n",
      "Voy en:  29\n",
      "Voy en:  30\n",
      "Voy en:  31\n",
      "Voy en:  32\n",
      "Voy en:  33\n",
      "Voy en:  34\n",
      "Voy en:  35\n",
      "Voy en:  36\n",
      "Voy en:  37\n",
      "Voy en:  38\n",
      "Voy en:  39\n",
      "Voy en:  40\n",
      "Voy en:  41\n",
      "Voy en:  42\n",
      "Voy en:  43\n",
      "Voy en:  44\n",
      "Voy en:  45\n",
      "Voy en:  46\n",
      "Voy en:  47\n",
      "Voy en:  48\n",
      "Voy en:  49\n",
      "Voy en:  50\n",
      "Voy en:  51\n",
      "Voy en:  52\n",
      "Voy en:  53\n",
      "Voy en:  54\n",
      "Voy en:  55\n",
      "Voy en:  56\n",
      "Voy en:  57\n",
      "Voy en:  58\n",
      "Voy en:  59\n",
      "Voy en:  60\n",
      "Voy en:  61\n",
      "Voy en:  62\n",
      "Voy en:  63\n",
      "Voy en:  64\n",
      "Voy en:  65\n",
      "Voy en:  66\n",
      "Voy en:  67\n",
      "Voy en:  68\n",
      "Voy en:  69\n",
      "Voy en:  70\n",
      "Voy en:  71\n",
      "Voy en:  72\n",
      "Voy en:  73\n",
      "Voy en:  74\n",
      "Voy en:  75\n",
      "Voy en:  76\n",
      "Voy en:  77\n",
      "Voy en:  78\n",
      "Voy en:  79\n",
      "Voy en:  80\n",
      "Voy en:  81\n",
      "Voy en:  82\n",
      "Voy en:  83\n",
      "Voy en:  84\n",
      "Voy en:  85\n",
      "Voy en:  86\n",
      "Voy en:  87\n",
      "Voy en:  88\n",
      "Voy en:  89\n",
      "Voy en:  90\n",
      "Voy en:  91\n",
      "Voy en:  92\n",
      "Voy en:  93\n",
      "Voy en:  94\n",
      "Voy en:  95\n",
      "Voy en:  96\n",
      "Voy en:  97\n",
      "Voy en:  98\n",
      "Voy en:  99\n",
      "Average Magnetization: -0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "# Simulation loop\n",
    "for t in range(100):\n",
    "    print(\"Voy en: \", t)\n",
    "    for i in range(N):\n",
    "        cost = cost_function(i, positions, spins)\n",
    "        state = update_Q(i, state, cost)\n",
    "        spins = spins.at[i].set(update_spin(i, spins, state, epsilon))\n",
    "        positions = positions.at[i].set(update_position(i, positions, spins, velocities))\n",
    "\n",
    "# Calculate average magnetization\n",
    "average_magnetization = jnp.mean(spins)\n",
    "print(f\"Average Magnetization: {average_magnetization}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1, -1,  1,  1, -1, -1, -1, -1, -1, -1], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.6450915 , -0.1382886 ,  9.124096  ,  5.3202004 , -0.59076375,\n",
       "       -0.263117  , -0.74900085, -0.7226927 , -0.2321845 , -0.13659248],      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
