{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ising model\n",
    "\n",
    "The Hamiltonian of the Ising chain is given by:\n",
    "\n",
    "$$\\hat{H}_I = J\\sum_{\\braket{i,j}}\\sigma_i \\sigma_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Class that creates an Ising environment\n",
    "class IsingEnv:\n",
    "    def __init__(self, N, T, key=jax.random.key(0)):\n",
    "        self.N = N # Number of spins in the Ising chain\n",
    "        self.T = T # Temperature of the system\n",
    "        self.key, _key = jax.random.split(key)\n",
    "        self.spins = jax.random.choice(_key, jnp.array([-1, 1]), shape=(self.N,)) # Array that keeps the spin states\n",
    "        \n",
    "    \n",
    "    # Function that resets the spin state to a random state\n",
    "    def reset(self):\n",
    "        self.key, _key = jax.random.split(self.key)\n",
    "        self.spins = jax.random.choice(_key, jnp.array([-1, 1]), shape=(self.N,))\n",
    "        return self.spins\n",
    "    \n",
    "    # Returns the energy of the current state\n",
    "    def energy(self, spins):\n",
    "        return -jnp.sum(spins * jnp.roll(spins, 1))\n",
    "    \n",
    "    # Flips spin at random\n",
    "    def step(self, action):\n",
    "        new_spins = self.spins.at[action].multiply(-1)\n",
    "        delta_E = self.energy(new_spins) - self.energy(self.spins)\n",
    "        \n",
    "        # Boltzmann reward (negative free energy difference)\n",
    "        reward = jnp.exp(-delta_E / self.T)\n",
    "        self.reward = reward\n",
    "        self.spins = new_spins\n",
    "        return new_spins, reward  # (state, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    num_actions: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=128)(x)\n",
    "        x = nn.relu(x)\n",
    "        q_values = nn.Dense(features=self.num_actions)(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements the DQN algorithm\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, key=jax.random.key(0), epsilon=0.1, learning_rate=1e-3, gamma=0.99, batch_size=32, buffer_size=10000):\n",
    "        self.state_dim = state_dim # Dimensionality of the state space (number of Isning spins)\n",
    "        self.num_actions = state_dim # Number of actions in the action space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma # Discount factor of future rewards\n",
    "        self.batch_size = batch_size # Number of samples to take from the replay buffer\n",
    "        self.buffer = deque(maxlen=buffer_size) # Replay buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.key, _key = jax.random.split(key)\n",
    "        \n",
    "        self.q_network = QNetwork(num_actions=state_dim) # Q-network\n",
    "        self.target_network = QNetwork(num_actions=state_dim) # Target network\n",
    "        \n",
    "        self.params = self.q_network.init(_key, jnp.ones(state_dim)) # Initialize the Q-network parameters\n",
    "        self.target_params = self.params # Initialize the target network parameters\n",
    "        \n",
    "        self.optimizer = optax.adam(learning_rate) # Adam optimizer\n",
    "        self.state = train_state.TrainState.create(apply_fn=self.q_network.apply, params=self.params, tx=self.optimizer) # Training state\n",
    "    \n",
    "    # Epsilon-greedy policy to select actions\n",
    "    def select_action(self, state):\n",
    "        self.key, _key, _key2 = jax.random.split(self.key, 3)\n",
    "        if jax.random.uniform(_key) < self.epsilon:\n",
    "            return jax.random.choice(_key2, jnp.arange(self.num_actions))\n",
    "        q_values = self.q_network.apply(self.state.params, state)\n",
    "        return jnp.argmax(q_values)\n",
    "    \n",
    "    # Store transition in the replay buffer\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    # Update the Q-network parameters (given that there is more than batch_size samples in the replay buffer)\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        states = jnp.array(states)\n",
    "        actions = jnp.array(actions)\n",
    "        rewards = jnp.array(rewards)\n",
    "        next_states = jnp.array(next_states)\n",
    "        \n",
    "        # Loss function\n",
    "        def loss_fn(params):\n",
    "            q_values = self.q_network.apply(params, states)\n",
    "            q_values = jnp.take_along_axis(q_values, actions[:, None], axis=1).squeeze()\n",
    "            \n",
    "            next_q_values = self.target_network.apply(self.target_params, next_states)\n",
    "            max_next_q_values = jnp.max(next_q_values, axis=1)\n",
    "            targets = rewards + self.gamma * max_next_q_values\n",
    "            \n",
    "            loss = jnp.mean((q_values - targets) ** 2)\n",
    "            return loss\n",
    "        \n",
    "        grads = jax.grad(loss_fn)(self.state.params)\n",
    "        self.state = self.state.apply_gradients(grads=grads)\n",
    "\n",
    "    # Update the target network parameters \n",
    "    def update_target_network(self):\n",
    "        self.target_params = self.state.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 17100.744140625\n",
      "Episode 2, Total Reward: 24994.0234375\n",
      "Episode 3, Total Reward: 26518.8828125\n",
      "Episode 4, Total Reward: 26097.9609375\n",
      "Episode 5, Total Reward: 26676.728515625\n",
      "Episode 6, Total Reward: 26362.01953125\n",
      "Episode 7, Total Reward: 26835.5546875\n",
      "Episode 8, Total Reward: 26254.82421875\n",
      "Episode 9, Total Reward: 26204.171875\n",
      "Episode 10, Total Reward: 26363.001953125\n",
      "Episode 11, Total Reward: 26625.09375\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m env \u001b[38;5;241m=\u001b[39m IsingEnv(N, T)\n\u001b[1;32m     27\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_dim\u001b[38;5;241m=\u001b[39mN)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(env, agent, num_samples, num_episodes, epsilon_start, epsilon_end, epsilon_decay)\u001b[0m\n\u001b[1;32m     10\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     12\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(state, action, reward, next_state)\n\u001b[0;32m---> 13\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     16\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mDQNAgent.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m actions \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(actions)\n\u001b[1;32m     44\u001b[0m rewards \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(rewards)\n\u001b[0;32m---> 45\u001b[0m next_states \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Loss function\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:5417\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device)\u001b[0m\n\u001b[1;32m   5415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   5416\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m-> 5417\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43melt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5418\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5419\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:4476\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype)\u001b[0m\n\u001b[1;32m   4474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll input arrays must have the same shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4475\u001b[0m   new_arrays\u001b[38;5;241m.\u001b[39mappend(expand_dims(a, axis))\n\u001b[0;32m-> 4476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_arrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:4640\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(arrays, axis, dtype)\u001b[0m\n\u001b[1;32m   4638\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 4640\u001b[0m   arrays_out \u001b[38;5;241m=\u001b[39m [\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4641\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[1;32m   4642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/lax/lax.py:652\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(operands, dimension)\u001b[0m\n\u001b[1;32m    650\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, Array):\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\n\u001b[0;32m--> 652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimension\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/core.py:438\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    436\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    437\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 438\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/core.py:442\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m    441\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m pop_level(trace\u001b[38;5;241m.\u001b[39mlevel):\n\u001b[0;32m--> 442\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/core.py:955\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m call_impl_with_key_reuse_checks(primitive, primitive\u001b[38;5;241m.\u001b[39mimpl, \u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax+torch_env/lib/python3.12/site-packages/jax/_src/dispatch.py:91\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     89\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to train the DQN agent\n",
    "def train_dqn(env, agent, num_samples=1_000, num_episodes=100, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
    "    env.epsilon = epsilon_start\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward = env.step(action)\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.update_target_network()\n",
    "        env.epsilon = max(epsilon_end, env.epsilon * epsilon_decay)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Example usage\n",
    "N = 5  # Number of spins\n",
    "T = 1.0  # Temperature\n",
    "env = IsingEnv(N, T)\n",
    "agent = DQNAgent(state_dim=N)\n",
    "\n",
    "train_dqn(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1,  1, -1, -1, -1], dtype=int32), Array([-1,  1, -1,  1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([1, 1, 1, 1, 1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1,  1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1, -1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1,  1, -1], dtype=int32), Array([ 1,  1,  1, -1, -1], dtype=int32), Array([ 1, -1,  1, -1, -1], dtype=int32), Array([-1, -1,  1, -1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1,  1, -1, -1, -1], dtype=int32), Array([-1,  1, -1,  1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1, -1,  1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1,  1, -1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1,  1, -1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1,  1, -1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([-1, -1, -1,  1, -1], dtype=int32), Array([-1, -1, -1, -1, -1], dtype=int32), Array([ 1, -1, -1, -1, -1], dtype=int32), Array([ 1, -1,  1, -1, -1], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "def sample_states(env, agent, num_samples=100):\n",
    "    states = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, _ = env.step(action)\n",
    "        states.append(next_state)\n",
    "        state = next_state\n",
    "    \n",
    "    return states\n",
    "\n",
    "# Example usage\n",
    "sampled_states = sample_states(env, agent, num_samples=100)\n",
    "print(sampled_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Energy: -2.6399998664855957\n"
     ]
    }
   ],
   "source": [
    "def mean_energy(env, sampled_states):\n",
    "    energies = jnp.array([env.energy(state) for state in sampled_states])\n",
    "    mean_energy = jnp.mean(energies)\n",
    "    return mean_energy\n",
    "\n",
    "# Example usage\n",
    "mean_energy_value = mean_energy(env, sampled_states)\n",
    "print(f\"Mean Energy: {mean_energy_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exact_ground_energy(N):\n",
    "    # For the 1D Ising model, the ground state energy is simply -N\n",
    "    return -N\n",
    "\n",
    "# Example usage\n",
    "ground_energy = exact_ground_energy(N)\n",
    "print(f\"Exact Ground Energy: {ground_energy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.randint(jax.random.PRNGKey(0), minval=0, maxval=10, shape=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DQNAgent.__init__() got an unexpected keyword argument 'num_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# Temperature\u001b[39;00m\n\u001b[1;32m     27\u001b[0m env \u001b[38;5;241m=\u001b[39m IsingEnv(N, T)\n\u001b[0;32m---> 28\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m train_dqn(env, agent)\n",
      "\u001b[0;31mTypeError\u001b[0m: DQNAgent.__init__() got an unexpected keyword argument 'num_actions'"
     ]
    }
   ],
   "source": [
    "def train_dqn(env, agent, num_episodes=1000, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state, epsilon)\n",
    "            next_state, reward = env.step(action)\n",
    "            done = jnp.array_equal(next_state, state)  # Example condition for done\n",
    "            \n",
    "            agent.store_transition(state, action, reward, next_state, done)\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.update_target_network()\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Example usage\n",
    "N = 10  # Number of spins\n",
    "T = 1.0  # Temperature\n",
    "env = IsingEnv(N, T)\n",
    "agent = DQNAgent(state_dim=(N,), num_actions=N)\n",
    "\n",
    "train_dqn(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    action_dim: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, spins, T):\n",
    "        x = jnp.concatenate([spins, jnp.array([T])])\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.relu(x)\n",
    "        return nn.Dense(self.action_dim)(x)\n",
    "\n",
    "# Initialize model\n",
    "N = 16  # Example size\n",
    "model = QNetwork(action_dim=N)\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones(N), 1.0)\n",
    "optimizer = optax.adam(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_CAPACITY = 10000\n",
    "GAMMA = 0.99\n",
    "TARGET_UPDATE = 100\n",
    "EPS_START, EPS_END, EPS_DECAY = 1.0, 0.1, 0.995\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self, model, params, N):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.target_params = params\n",
    "        self.optimizer = optax.adam(1e-3)\n",
    "        self.opt_state = self.optimizer.init(params)\n",
    "        self.replay_buffer = deque(maxlen=BUFFER_CAPACITY)\n",
    "        self.N = N\n",
    "        self.epsilon = EPS_START\n",
    "        self.metrics = {\n",
    "            'episode_rewards': [],\n",
    "            'avg_energy': [],\n",
    "            'epsilon': []\n",
    "        }\n",
    "\n",
    "    @jax.jit\n",
    "    def _update(self, params, target_params, opt_state, batch):\n",
    "        states, Ts, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        def loss_fn(params):\n",
    "            q = self.model.apply(params, states, Ts)\n",
    "            q = q[jnp.arange(q.shape[0]), actions]\n",
    "            \n",
    "            next_q = self.model.apply(target_params, next_states, Ts)\n",
    "            max_next_q = jnp.max(next_q, axis=1)\n",
    "            targets = rewards + GAMMA * max_next_q * (1 - dones)\n",
    "            \n",
    "            return jnp.mean((q - targets)**2)\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "        updates, new_opt_state = self.optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, new_opt_state, loss\n",
    "\n",
    "    def train_step(self, transition):\n",
    "        self.replay_buffer.append(transition)\n",
    "        \n",
    "        if len(self.replay_buffer) >= BATCH_SIZE:\n",
    "            indices = np.random.choice(len(self.replay_buffer), BATCH_SIZE)\n",
    "            batch = [self.replay_buffer[i] for i in indices]\n",
    "            \n",
    "            # Explicit unpacking for clarity\n",
    "            states = jnp.stack([t[0] for t in batch])\n",
    "            actions = jnp.array([t[1] for t in batch])\n",
    "            rewards = jnp.array([t[2] for t in batch])\n",
    "            next_states = jnp.stack([t[3] for t in batch])\n",
    "            dones = jnp.array([t[4] for t in batch])\n",
    "            Ts = jnp.array([t[5] for t in batch])\n",
    "            \n",
    "            self.params, self.opt_state, _ = self._update(\n",
    "                self.params, self.target_params, self.opt_state,\n",
    "                (states, Ts, actions, rewards, next_states, dones)\n",
    "            )\n",
    "            \n",
    "            self.epsilon = max(EPS_END, self.epsilon * EPS_DECAY)\n",
    "            \n",
    "            if self.train_steps % TARGET_UPDATE == 0:\n",
    "                self.target_params = self.params\n",
    "            self.train_steps += 1\n",
    "            \n",
    "    def record_metrics(self, episode_reward, avg_energy):\n",
    "        self.metrics['episode_rewards'].append(episode_reward)\n",
    "        self.metrics['avg_energy'].append(avg_energy)\n",
    "        self.metrics['epsilon'].append(self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error interpreting argument to <function DQNTrainer._update at 0x17f8cef20> as an abstract array. The problematic value is of type <class '__main__.DQNTrainer'> and was passed to the function at path self.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax-env/lib/python3.12/site-packages/jax/_src/api_util.py:604\u001b[0m, in \u001b[0;36mshaped_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    603\u001b[0m handler \u001b[38;5;241m=\u001b[39m _shaped_abstractify_handlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtype\u001b[39m(x), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m handler(x) \u001b[38;5;28;01mif\u001b[39;00m handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_shaped_abstractify_slow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax-env/lib/python3.12/site-packages/jax/_src/api_util.py:596\u001b[0m, in \u001b[0;36m_shaped_abstractify_slow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    597\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot interpret value of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array; it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    598\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not have a dtype attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m core\u001b[38;5;241m.\u001b[39mShapedArray(np\u001b[38;5;241m.\u001b[39mshape(x), dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret value of type <class '__main__.DQNTrainer'> as an abstract array; it does not have a dtype attribute",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Reward \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Energy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_energy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | ε \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 34\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(num_episodes)\u001b[0m\n\u001b[1;32m     32\u001b[0m next_state, reward, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     33\u001b[0m energy \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39menergy(next_state)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     37\u001b[0m energy_accum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m energy\n",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m, in \u001b[0;36mDQNTrainer.train_step\u001b[0;34m(self, transition)\u001b[0m\n\u001b[1;32m     61\u001b[0m dones \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([t[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     62\u001b[0m Ts \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([t[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(EPS_END, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon \u001b[38;5;241m*\u001b[39m EPS_DECAY)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_steps \u001b[38;5;241m%\u001b[39m TARGET_UPDATE \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "    \u001b[0;31m[... skipping hidden 4 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/jax-env/lib/python3.12/site-packages/jax/_src/pjit.py:622\u001b[0m, in \u001b[0;36m_infer_params_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    620\u001b[0m       arg_description \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdbg\u001b[38;5;241m.\u001b[39marg_names[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dbg\n\u001b[1;32m    621\u001b[0m                          \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflattened argument number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 622\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError interpreting argument to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The problematic value is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and was passed to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the function at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis typically means that a jit-wrapped function was called with a non-array\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m argument, and this argument was not marked as static using the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m static_argnums or static_argnames parameters of jax.jit.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    629\u001b[0m       ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    631\u001b[0m   in_type \u001b[38;5;241m=\u001b[39m in_avals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(avals)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Error interpreting argument to <function DQNTrainer._update at 0x17f8cef20> as an abstract array. The problematic value is of type <class '__main__.DQNTrainer'> and was passed to the function at path self.\nThis typically means that a jit-wrapped function was called with a non-array argument, and this argument was not marked as static using the static_argnums or static_argnames parameters of jax.jit."
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from flax import linen as nn\n",
    "\n",
    "def run_training(num_episodes=1000):\n",
    "    N = 16\n",
    "    model = QNetwork(action_dim=N)\n",
    "    params = model.init(jax.random.PRNGKey(0), jnp.ones(N), 1.0)\n",
    "    trainer = DQNTrainer(model, params, N)\n",
    "    max_steps = 100  # Steps per episode\n",
    "    \n",
    "    with open(f'metrics_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['Episode', 'AvgReward', 'AvgEnergy', 'Epsilon'])\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            key = jax.random.PRNGKey(ep)\n",
    "            T = np.random.uniform(0.5, 2.0)\n",
    "            env = IsingEnv(N, T)\n",
    "            state = env.reset(key)\n",
    "            total_reward = 0.0\n",
    "            energy_accum = 0.0\n",
    "            \n",
    "            for _ in range(max_steps):\n",
    "                if np.random.rand() < trainer.epsilon:\n",
    "                    action = np.random.randint(N)\n",
    "                else:\n",
    "                    q_vals = model.apply(trainer.params, state, T)\n",
    "                    action = jnp.argmax(q_vals).item()\n",
    "                \n",
    "                next_state, reward, _, _ = env.step(action)\n",
    "                energy = env.energy(next_state)\n",
    "                trainer.train_step((state, action, reward, next_state, False, T))\n",
    "                \n",
    "                total_reward += reward\n",
    "                energy_accum += energy\n",
    "                state = next_state\n",
    "\n",
    "            avg_reward = total_reward / max_steps\n",
    "            avg_energy = energy_accum / max_steps\n",
    "            trainer.record_metrics(avg_reward, avg_energy)\n",
    "            \n",
    "            # Log to CSV every episode\n",
    "            writer.writerow([ep, avg_reward, avg_energy, trainer.epsilon])\n",
    "            \n",
    "            # Print summary every 50 episodes\n",
    "            if ep % 50 == 0:\n",
    "                print(f\"Ep {ep}: Reward {avg_reward:.3f} | Energy {avg_energy:.1f} | ε {trainer.epsilon:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training(num_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('metrics_*.csv')\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(131)\n",
    "plt.plot(df['AvgReward'])\n",
    "plt.title('Average Reward')\n",
    "plt.subplot(132)\n",
    "plt.plot(df['AvgEnergy'])\n",
    "plt.title('System Energy')\n",
    "plt.subplot(133)\n",
    "plt.plot(df['Epsilon'])\n",
    "plt.title('Exploration Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax+torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
