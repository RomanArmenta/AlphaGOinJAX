{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from typing import Sequence, Callable, Tuple, Union, Dict, Any\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_density(x):\n",
    "    \"\"\"Example: 2-component Gaussian mixture\"\"\"\n",
    "    mu1 = jnp.array([-5.0])\n",
    "    mu2 = jnp.array([5.0])\n",
    "    sigma = 1.0\n",
    "    \n",
    "    log_p1 = -0.5 * jnp.sum(((x - mu1) / sigma) ** 2)\n",
    "    log_p2 = -0.5 * jnp.sum(((x - mu2) / sigma) ** 2)\n",
    "    return jnp.logaddexp(log_p1, log_p2)  # log(exp(log_p1) + exp(log_p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amh(target_log_pdf, key: jax.random.PRNGKey, d: int, m: int, gamma_init: float, alpha_star: float = 0.234):\n",
    "    \"\"\"\n",
    "    Adaptive Metropolis-Hastings algorithm for warm-up\n",
    "    \n",
    "    Args:\n",
    "        target_log_pdf: Function that returns log probability of target distribution\n",
    "        key: JAX random key\n",
    "        d: Dimension of the parameter space\n",
    "        m: Number of iterations\n",
    "        gamma_init: Initial adaptation rate\n",
    "        alpha_star: Target acceptance rate\n",
    "        \n",
    "    Returns:\n",
    "        samples, means, covariances, scale factors, acceptance indicators\n",
    "    \"\"\"\n",
    "    x = jnp.zeros((d,))\n",
    "    mu = jnp.zeros((d,))\n",
    "    sigma = jnp.eye(d)\n",
    "    lam = 1.0\n",
    "\n",
    "    samples = jnp.zeros((m, d))\n",
    "    means = jnp.zeros((m, d))\n",
    "    sigmas = jnp.zeros((m, d, d)).at[0].set(jnp.eye(d))\n",
    "    lambdas = jnp.zeros((m,))\n",
    "    accept_status = jnp.zeros((m,), dtype=bool)\n",
    "\n",
    "    def gamma_(g, i):\n",
    "        return g * (1 - (i / m))\n",
    "\n",
    "    def step(carry, i):\n",
    "        x, gamma, mu, sigma, lam, key = carry\n",
    "        \n",
    "        sample_key, uni_key, key = jax.random.split(key, 3)\n",
    "\n",
    "        # Sample from the proposal distribution\n",
    "        x_prop = x + jax.random.multivariate_normal(sample_key, mean=jnp.zeros(d), cov=lam * sigma)\n",
    "\n",
    "        # Compute the acceptance probability (in log space)\n",
    "        log_alpha = target_log_pdf(x_prop) - target_log_pdf(x)\n",
    "        alpha = jnp.minimum(0.0, log_alpha)\n",
    "\n",
    "        # Accept or reject the new sample\n",
    "        accept = jnp.log(jax.random.uniform(uni_key)) < alpha\n",
    "        x_new = jnp.where(accept, x_prop, x)\n",
    "\n",
    "        # Update the parameters\n",
    "        mu_new = mu + gamma * (x_new - mu)\n",
    "        sigma_new = sigma + gamma * ((x_new - mu)[:, None] @ (x_new - mu)[None, :] - sigma)\n",
    "        log_lam = jnp.log(lam) + gamma * (jnp.exp(alpha) - alpha_star)\n",
    "        lam_new = jnp.exp(log_lam)\n",
    "\n",
    "        gamma = gamma_(gamma, i)\n",
    "\n",
    "        outputs = (x_new, mu_new, sigma_new, lam_new, accept)\n",
    "\n",
    "        return (x_new, gamma, mu_new, sigma_new, lam_new, key), outputs\n",
    "\n",
    "    carry = (x, gamma_init, mu, sigma, lam, key)\n",
    "    _, (xs, mus, Sigmas, lambdas_, acc) = jax.lax.scan(\n",
    "        step, carry, jnp.arange(1, m)\n",
    "    )\n",
    "\n",
    "    # Add initial values\n",
    "    samples = samples.at[1:].set(xs)\n",
    "    means = means.at[1:].set(mus)\n",
    "    sigmas = sigmas.at[1:].set(Sigmas)\n",
    "    lambdas = lambdas.at[1:].set(lambdas_)\n",
    "    accept_status = accept_status.at[1:].set(acc)\n",
    "\n",
    "    return samples, means, sigmas, lambdas, accept_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNetwork(nn.Module):\n",
    "    \"\"\"Neural network for the proposal distribution\"\"\"\n",
    "    features: Sequence[int]\n",
    "    mean: jnp.ndarray\n",
    "    sigma: jnp.ndarray\n",
    "    l: float = 10.0\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Computing square root of the covariance matrix\n",
    "        sigma_reg = self.sigma + 1e-6 * jnp.eye(self.sigma.shape[0])  # Regularization\n",
    "        eigvals, eigvecs = jnp.linalg.eigh(sigma_reg)\n",
    "        inv_sqrt_sigma = eigvecs @ jnp.diag(1.0 / jnp.sqrt(jnp.maximum(eigvals, 1e-6))) @ eigvecs.T\n",
    "        sqrt_sigma = eigvecs @ jnp.diag(jnp.sqrt(jnp.maximum(eigvals, 1e-6))) @ eigvecs.T\n",
    "        \n",
    "        # Neural network for nu_theta\n",
    "        h = x\n",
    "        for feat in self.features:\n",
    "            h = nn.relu(nn.Dense(feat, kernel_init=nn.initializers.xavier_uniform())(h))\n",
    "        nu_x = nn.Dense(self.sigma.shape[0], kernel_init=nn.initializers.xavier_uniform())(h)\n",
    "        \n",
    "        # Define psi (mean + transformation from neural network)\n",
    "        psi = self.mean + sqrt_sigma @ nu_x\n",
    "        \n",
    "        # Apply the regularity-ensuring transformation\n",
    "        eta = jnp.sum(jnp.square(inv_sqrt_sigma @ (x - self.mean))) / self.l**2\n",
    "        gamma = self.compute_gamma(eta)\n",
    "        \n",
    "        # Final phi output\n",
    "        phi = psi + gamma * (x - psi)\n",
    "        return phi\n",
    "    \n",
    "    def compute_gamma(self, eta):\n",
    "        \"\"\"Compute the gamma function for regularity\"\"\"\n",
    "        return jnp.where(eta <= 0.5, 0.0,\n",
    "               jnp.where(eta >= 1.0, 1.0,\n",
    "               1.0 / (1.0 + jnp.exp(-(4.0*eta - 3.0)/(4.0*eta*eta - 6.0*eta + 2.0)))))\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"Actor network for RLMH\"\"\"\n",
    "    features: Sequence[int]\n",
    "    mean: jnp.ndarray\n",
    "    sigma: jnp.ndarray\n",
    "    l: float = 10.0\n",
    "    \n",
    "    def setup(self):\n",
    "        self.phi_net = PhiNetwork(features=self.features, mean=self.mean, sigma=self.sigma, l=self.l)\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        d = self.mean.shape[0]\n",
    "        x = state[:d]\n",
    "        x_prop = state[d:]\n",
    "        \n",
    "        phi_x = self.phi_net(x)\n",
    "        phi_prop = self.phi_net(x_prop)\n",
    "        \n",
    "        return jnp.concatenate([phi_x, phi_prop], axis=0)\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"Critic network for RLMH\"\"\"\n",
    "    features: Sequence[int]\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, state, action):\n",
    "        # Concatenate state and action\n",
    "        x = jnp.concatenate([state, action], axis=0)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        for feat in self.features:\n",
    "            x = nn.relu(nn.Dense(feat, kernel_init=nn.initializers.xavier_uniform())(x))\n",
    "        \n",
    "        # Output Q-value\n",
    "        q_value = nn.Dense(1, kernel_init=nn.initializers.xavier_uniform())(x)\n",
    "        return q_value.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_laplace(y, sigma_rootinv, mean):\n",
    "    \"\"\"Log density of Laplace distribution\n",
    "    \n",
    "    Args:\n",
    "        y: Proposal sample\n",
    "        sigma_rootinv: Inverse square root of covariance\n",
    "        mean: Mean of the distribution\n",
    "    \n",
    "    Returns:\n",
    "        Log probability\n",
    "    \"\"\"\n",
    "    return -jnp.linalg.norm(sigma_rootinv @ (y - mean), ord=1)\n",
    "\n",
    "@jax.jit\n",
    "def sample_laplace(key, mean, sigma):\n",
    "    \"\"\"Sample from Laplace distribution\n",
    "    \n",
    "    Args:\n",
    "        key: JAX PRNG key\n",
    "        mean: Mean of the distribution\n",
    "        sigma: Covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "        Sample from Laplace distribution\n",
    "    \"\"\"\n",
    "    d = mean.shape[0]\n",
    "    \n",
    "    # Compute the inverse square root of sigma\n",
    "    eigvals, eigvecs = jnp.linalg.eigh(sigma)\n",
    "    sigma_rootinv = eigvecs @ jnp.diag(1.0 / jnp.sqrt(jnp.maximum(eigvals, 1e-6))) @ eigvecs.T\n",
    "    \n",
    "    # Generate standard exponential random variables\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    z1 = jax.random.exponential(key1, shape=(d,))\n",
    "    z2 = jax.random.exponential(key2, shape=(d,))\n",
    "    \n",
    "    # Difference of exponentials gives Laplace\n",
    "    z = z1 - z2\n",
    "    \n",
    "    # Transform to get the right mean and covariance\n",
    "    return mean + jnp.linalg.solve(sigma_rootinv, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DDPG\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_size=100000):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.states = np.zeros((max_size, state_dim))\n",
    "        self.actions = np.zeros((max_size, action_dim))\n",
    "        self.rewards = np.zeros((max_size, 1))\n",
    "        self.next_states = np.zeros((max_size, state_dim))\n",
    "        \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        self.states[self.ptr] = state\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.next_states[self.ptr] = next_state\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            jnp.array(self.states[ind]),\n",
    "            jnp.array(self.actions[ind]),\n",
    "            jnp.array(self.rewards[ind]),\n",
    "            jnp.array(self.next_states[ind])\n",
    "        )\n",
    "\n",
    "\n",
    "class DDPGTrainState:\n",
    "    \"\"\"Training state for DDPG algorithm\"\"\"\n",
    "    \n",
    "    def __init__(self, actor_params, critic_params, target_actor_params, \n",
    "                 target_critic_params, actor_opt_state, critic_opt_state):\n",
    "        self.actor_params = actor_params\n",
    "        self.critic_params = critic_params\n",
    "        self.target_actor_params = target_actor_params\n",
    "        self.target_critic_params = target_critic_params\n",
    "        self.actor_opt_state = actor_opt_state\n",
    "        self.critic_opt_state = critic_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLMH:\n",
    "    \"\"\"Reinforcement Learning Metropolis-Hastings (RLMH) algorithm\n",
    "    \n",
    "    This class implements the RLMH algorithm described in the paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_log_density, d, actor_features, critic_features, \n",
    "                 batch_size=64, gamma=0.99, tau=0.005, buffer_size=100000, \n",
    "                 actor_lr=1e-4, critic_lr=1e-3):\n",
    "        \"\"\"Initialize RLMH\n",
    "        \n",
    "        Args:\n",
    "            target_log_density: Log density function to sample from\n",
    "            d: Dimension of the parameter space\n",
    "            actor_features: Hidden layer sizes for actor network\n",
    "            critic_features: Hidden layer sizes for critic network\n",
    "            batch_size: Batch size for training\n",
    "            gamma: Discount factor for DDPG\n",
    "            tau: Soft update coefficient\n",
    "            buffer_size: Size of the replay buffer\n",
    "            actor_lr: Learning rate for actor\n",
    "            critic_lr: Learning rate for critic\n",
    "        \"\"\"\n",
    "        self.target_log_density = target_log_density\n",
    "        self.d = d\n",
    "        self.actor_features = actor_features\n",
    "        self.critic_features = critic_features\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(2*d, 2*d, buffer_size)\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        \n",
    "        # Will be initialized after warm-up\n",
    "        self.mean = None\n",
    "        self.sigma = None\n",
    "        self.actor = None\n",
    "        self.critic = None\n",
    "        self.train_state = None\n",
    "        self.current_state = None\n",
    "        \n",
    "    def warmup(self, key, m=10000, gamma_init=0.1, alpha_star=0.234):\n",
    "        \"\"\"Run AMH to get initial samples and adaptation parameters\n",
    "        \n",
    "        Args:\n",
    "            key: JAX PRNG key\n",
    "            m: Number of warm-up iterations\n",
    "            gamma_init: Initial adaptation rate\n",
    "            alpha_star: Target acceptance rate\n",
    "            \n",
    "        Returns:\n",
    "            Samples from the warm-up phase\n",
    "        \"\"\"\n",
    "        print(\"Running AMH warm-up...\")\n",
    "        \n",
    "        samples, means, sigmas, lambdas, accept_status = amh(\n",
    "            self.target_log_density, key, self.d, m, gamma_init, alpha_star\n",
    "        )\n",
    "        \n",
    "        # Store the mean and covariance from warm-up\n",
    "        self.mean = means[-1]\n",
    "        self.sigma = sigmas[-1]\n",
    "        \n",
    "        print(f\"Warm-up acceptance rate: {jnp.mean(accept_status):.3f}\")\n",
    "        print(f\"Final lambda: {lambdas[-1]:.5f}\")\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def initialize_networks(self, key):\n",
    "        \"\"\"Initialize actor and critic networks after warm-up\n",
    "        \n",
    "        Args:\n",
    "            key: JAX PRNG key\n",
    "        \"\"\"\n",
    "        if self.mean is None or self.sigma is None:\n",
    "            raise ValueError(\"Must run warm-up before initializing networks\")\n",
    "        \n",
    "        key1, key2 = jax.random.split(key)\n",
    "        \n",
    "        # Initialize actor network\n",
    "        self.actor = ActorNetwork(\n",
    "            features=self.actor_features,\n",
    "            mean=self.mean,\n",
    "            sigma=self.sigma\n",
    "        )\n",
    "        \n",
    "        # Initialize critic network\n",
    "        self.critic = CriticNetwork(\n",
    "            features=self.critic_features\n",
    "        )\n",
    "        \n",
    "        # Initial state for both networks\n",
    "        dummy_state = jnp.zeros(2 * self.d)\n",
    "        dummy_action = jnp.zeros(2 * self.d)\n",
    "        \n",
    "        actor_params = self.actor.init(key1, dummy_state)\n",
    "        critic_params = self.critic.init(key2, dummy_state, dummy_action)\n",
    "        \n",
    "        # Initialize optimizers\n",
    "        actor_optimizer = optax.adam(learning_rate=self.actor_lr)\n",
    "        critic_optimizer = optax.adam(learning_rate=self.critic_lr)\n",
    "        \n",
    "        actor_opt_state = actor_optimizer.init(actor_params)\n",
    "        critic_opt_state = critic_optimizer.init(critic_params)\n",
    "        \n",
    "        # Initialize target networks with same parameters\n",
    "        target_actor_params = actor_params\n",
    "        target_critic_params = critic_params\n",
    "        \n",
    "        # Create training state\n",
    "        self.train_state = DDPGTrainState(\n",
    "            actor_params=actor_params,\n",
    "            critic_params=critic_params,\n",
    "            target_actor_params=target_actor_params,\n",
    "            target_critic_params=target_critic_params,\n",
    "            actor_opt_state=actor_opt_state,\n",
    "            critic_opt_state=critic_opt_state\n",
    "        )\n",
    "        \n",
    "        # Set up optimizers as functions\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        \n",
    "        # Initialize JIT-compiled functions\n",
    "        self.init_jitted_functions()\n",
    "    \n",
    "    def initialize_state(self, key, x_init=None):\n",
    "        \"\"\"Initialize state for RLMH\n",
    "        \n",
    "        Args:\n",
    "            key: JAX PRNG key\n",
    "            x_init: Initial state (optional)\n",
    "        \"\"\"\n",
    "        if x_init is None:\n",
    "            x_init = jnp.zeros(self.d)\n",
    "        \n",
    "        # Generate proposal using Laplace distribution\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x_prop = sample_laplace(subkey, self.mean, self.sigma)\n",
    "        \n",
    "        # Set current state\n",
    "        self.current_state = jnp.concatenate([x_init, x_prop])\n",
    "        \n",
    "        return key\n",
    "    \n",
    "    #@jax.jit\n",
    "    def _compute_acceptance_probability(self, x, x_prop, phi_x, phi_prop):\n",
    "        \"\"\"Compute Metropolis-Hastings acceptance probability\n",
    "        \n",
    "        Args:\n",
    "            x: Current state\n",
    "            x_prop: Proposed state\n",
    "            phi_x: Mean of proposal for x\n",
    "            phi_prop: Mean of proposal for x_prop\n",
    "            \n",
    "        Returns:\n",
    "            Log acceptance probability\n",
    "        \"\"\"\n",
    "        # Log probability ratio of target distribution\n",
    "        log_prob_ratio = self.target_log_density(x_prop) - self.target_log_density(x)\n",
    "        \n",
    "        # Log proposal ratio for Laplace distribution\n",
    "        log_prop_ratio = log_laplace(x, self.sigma, phi_prop) - log_laplace(x_prop, self.sigma, phi_x)\n",
    "        \n",
    "        # Log acceptance probability\n",
    "        return jnp.minimum(0.0, log_prob_ratio + log_prop_ratio)\n",
    "    \n",
    "    #@jax.jit\n",
    "    def _compute_reward(self, x, x_prop, log_alpha):\n",
    "        \"\"\"Compute reward for RLMH\n",
    "        \n",
    "        Args:\n",
    "            x: Current state\n",
    "            x_prop: Proposed state\n",
    "            log_alpha: Log acceptance probability\n",
    "            \n",
    "        Returns:\n",
    "            Reward\n",
    "        \"\"\"\n",
    "        # Reward as defined in the paper\n",
    "        return 2.0 * jnp.log(jnp.linalg.norm(x - x_prop) + 1e-10) + log_alpha\n",
    "    \n",
    "    # Create JIT-compiled versions separately\n",
    "    def init_jitted_functions(self):\n",
    "        \"\"\"Initialize JIT-compiled functions\"\"\"\n",
    "        # We need to use closures to capture self\n",
    "        target_log_density = self.target_log_density\n",
    "        sigma = self.sigma\n",
    "        \n",
    "        @jax.jit\n",
    "        def _jit_compute_acceptance_probability(x, x_prop, phi_x, phi_prop):\n",
    "            # Log probability ratio of target distribution\n",
    "            log_prob_ratio = target_log_density(x_prop) - target_log_density(x)\n",
    "            \n",
    "            # Log proposal ratio for Laplace distribution\n",
    "            log_prop_ratio = log_laplace(x, sigma, phi_prop) - log_laplace(x_prop, sigma, phi_x)\n",
    "            \n",
    "            # Log acceptance probability\n",
    "            return jnp.minimum(0.0, log_prob_ratio + log_prop_ratio)\n",
    "        \n",
    "        @jax.jit\n",
    "        def _jit_compute_reward(x, x_prop, log_alpha):\n",
    "            # Reward as defined in the paper\n",
    "            return 2.0 * jnp.log(jnp.linalg.norm(x - x_prop) + 1e-10) + log_alpha\n",
    "        \n",
    "        self._jit_compute_acceptance_probability = _jit_compute_acceptance_probability\n",
    "        self._jit_compute_reward = _jit_compute_reward\n",
    "    \n",
    "    def step(self, key):\n",
    "        \"\"\"Take one step of RLMH\"\"\"\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        \n",
    "        # Get current state\n",
    "        state = self.current_state\n",
    "        x = state[:self.d]\n",
    "        x_prop = state[self.d:]\n",
    "        \n",
    "        # Get action from actor\n",
    "        action = self.actor.apply(self.train_state.actor_params, state)\n",
    "        phi_x = action[:self.d]\n",
    "        phi_prop = action[self.d:]\n",
    "        \n",
    "        # Check if we have JIT-compiled functions\n",
    "        if not hasattr(self, '_jit_compute_acceptance_probability'):\n",
    "            self.init_jitted_functions()\n",
    "        \n",
    "        # Use JIT-compiled functions\n",
    "        log_alpha = self._jit_compute_acceptance_probability(x, x_prop, phi_x, phi_prop)\n",
    "        reward = self._jit_compute_reward(x, x_prop, log_alpha)\n",
    "        \n",
    "        # Accept/reject step\n",
    "        accept = jnp.log(jax.random.uniform(key1)) < log_alpha\n",
    "        x_next = jnp.where(accept, x_prop, x)\n",
    "        \n",
    "        # Generate next proposal\n",
    "        x_prop_next = sample_laplace(key2, phi_x, self.sigma)\n",
    "        \n",
    "        # Update current state\n",
    "        next_state = jnp.concatenate([x_next, x_prop_next])\n",
    "        \n",
    "        # Store experience in replay buffer\n",
    "        self.replay_buffer.add(state, action, reward, next_state)\n",
    "        \n",
    "        # Update current state\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return key3, reward, accept\n",
    "    \n",
    "    def update_networks(self):\n",
    "        \"\"\"Update actor and critic networks using DDPG\n",
    "        \n",
    "        Returns:\n",
    "            Updated train state, actor loss, critic loss\n",
    "        \"\"\"\n",
    "        if self.replay_buffer.size < self.batch_size:\n",
    "            return self.train_state, 0.0, 0.0\n",
    "        \n",
    "        # Sample a batch from replay buffer\n",
    "        states, actions, rewards, next_states = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Compute critic loss\n",
    "        def critic_loss_fn(critic_params):\n",
    "            # Q values for current state-action pairs\n",
    "            q_values = jax.vmap(self.critic.apply, in_axes=(None, 0, 0))(\n",
    "                critic_params, states, actions\n",
    "            )\n",
    "            \n",
    "            # Next actions from target actor\n",
    "            next_actions = jax.vmap(self.actor.apply, in_axes=(None, 0))(\n",
    "                self.train_state.target_actor_params, next_states\n",
    "            )\n",
    "            \n",
    "            # Q values for next state-action pairs from target critic\n",
    "            next_q_values = jax.vmap(self.critic.apply, in_axes=(None, 0, 0))(\n",
    "                self.train_state.target_critic_params, next_states, next_actions\n",
    "            )\n",
    "            \n",
    "            # Target Q values\n",
    "            target_q = rewards + self.gamma * next_q_values\n",
    "            \n",
    "            # Mean squared error loss\n",
    "            return jnp.mean(jnp.square(q_values - target_q))\n",
    "        \n",
    "        # Compute gradients for critic\n",
    "        critic_grad_fn = jax.value_and_grad(critic_loss_fn)\n",
    "        critic_loss, critic_grads = critic_grad_fn(self.train_state.critic_params)\n",
    "        \n",
    "        # Update critic\n",
    "        critic_updates, critic_opt_state = self.critic_optimizer.update(\n",
    "            critic_grads, self.train_state.critic_opt_state\n",
    "        )\n",
    "        critic_params = optax.apply_updates(self.train_state.critic_params, critic_updates)\n",
    "        \n",
    "        # Compute actor loss\n",
    "        def actor_loss_fn(actor_params):\n",
    "            # Actions from actor\n",
    "            pred_actions = jax.vmap(self.actor.apply, in_axes=(None, 0))(\n",
    "                actor_params, states\n",
    "            )\n",
    "            \n",
    "            # Q values from critic\n",
    "            q_values = jax.vmap(self.critic.apply, in_axes=(None, 0, 0))(\n",
    "                self.train_state.critic_params, states, pred_actions\n",
    "            )\n",
    "            \n",
    "            # Negative mean Q value (we want to maximize Q)\n",
    "            return -jnp.mean(q_values)\n",
    "        \n",
    "        # Compute gradients for actor\n",
    "        actor_grad_fn = jax.value_and_grad(actor_loss_fn)\n",
    "        actor_loss, actor_grads = actor_grad_fn(self.train_state.actor_params)\n",
    "        \n",
    "        # Update actor\n",
    "        actor_updates, actor_opt_state = self.actor_optimizer.update(\n",
    "            actor_grads, self.train_state.actor_opt_state\n",
    "        )\n",
    "        actor_params = optax.apply_updates(self.train_state.actor_params, actor_updates)\n",
    "        \n",
    "        # Soft update for target networks\n",
    "        target_actor_params = jax.tree_map(\n",
    "            lambda x, y: (1 - self.tau) * x + self.tau * y,\n",
    "            self.train_state.target_actor_params,\n",
    "            actor_params\n",
    "        )\n",
    "        \n",
    "        target_critic_params = jax.tree_map(\n",
    "            lambda x, y: (1 - self.tau) * x + self.tau * y,\n",
    "            self.train_state.target_critic_params,\n",
    "            critic_params\n",
    "        )\n",
    "        \n",
    "        # Update training state\n",
    "        self.train_state = DDPGTrainState(\n",
    "            actor_params=actor_params,\n",
    "            critic_params=critic_params,\n",
    "            target_actor_params=target_actor_params,\n",
    "            target_critic_params=target_critic_params,\n",
    "            actor_opt_state=actor_opt_state,\n",
    "            critic_opt_state=critic_opt_state\n",
    "        )\n",
    "        \n",
    "        return self.train_state, actor_loss, critic_loss\n",
    "    \n",
    "    def train(self, key, num_iterations=10000, update_every=10, log_every=1000):\n",
    "        \"\"\"Train RLMH\n",
    "        \n",
    "        Args:\n",
    "            key: JAX PRNG key\n",
    "            num_iterations: Number of training iterations\n",
    "            update_every: Update networks every N steps\n",
    "            log_every: Log stats every N steps\n",
    "            \n",
    "        Returns:\n",
    "            Samples, rewards, acceptance rates\n",
    "        \"\"\"\n",
    "        if self.train_state is None:\n",
    "            raise ValueError(\"Must initialize networks before training\")\n",
    "        \n",
    "        samples = np.zeros((num_iterations, self.d))\n",
    "        rewards = np.zeros(num_iterations)\n",
    "        accepts = np.zeros(num_iterations, dtype=bool)\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            # Take a step\n",
    "            key, reward, accept = self.step(key)\n",
    "            \n",
    "            # Store results\n",
    "            samples[i] = self.current_state[:self.d]\n",
    "            rewards[i] = reward\n",
    "            accepts[i] = accept\n",
    "            \n",
    "            # Update networks\n",
    "            if i % update_every == 0:\n",
    "                _, actor_loss, critic_loss = self.update_networks()\n",
    "            \n",
    "            # Log progress\n",
    "            if i % log_every == 0:\n",
    "                recent_accept_rate = np.mean(accepts[max(0, i-log_every):i+1])\n",
    "                print(f\"Iteration {i}: Accept rate = {recent_accept_rate:.3f}, \" +\n",
    "                      f\"Average reward = {np.mean(rewards[max(0, i-log_every):i+1]):.3f}\")\n",
    "        \n",
    "        return samples, rewards, accepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rlmh_experiment(target_log_density, d=1, num_warmup=10000, num_train=50000):\n",
    "    \"\"\"Run a complete RLMH experiment\n",
    "    \n",
    "    Args:\n",
    "        target_log_density: Log density function to sample from\n",
    "        d: Dimension of the parameter space\n",
    "        num_warmup: Number of warm-up iterations\n",
    "        num_train: Number of training iterations\n",
    "        \n",
    "    Returns:\n",
    "        RLMH instance, samples, rewards, acceptance rates\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    \n",
    "    # Create RLMH instance\n",
    "    rlmh = RLMH(\n",
    "        target_log_density=target_log_density,\n",
    "        d=d,\n",
    "        actor_features=[64, 64],\n",
    "        critic_features=[64, 64]\n",
    "    )\n",
    "    \n",
    "    # Run warm-up phase\n",
    "    warmup_samples = rlmh.warmup(subkey, m=num_warmup)\n",
    "    \n",
    "    # Initialize networks\n",
    "    key, subkey = jax.random.split(key)\n",
    "    rlmh.initialize_networks(subkey)\n",
    "    \n",
    "    # Initialize state with last sample from warm-up\n",
    "    key, subkey = jax.random.split(key)\n",
    "    rlmh.initialize_state(subkey, x_init=warmup_samples[-1])\n",
    "    \n",
    "    # Train RLMH\n",
    "    key, subkey = jax.random.split(key)\n",
    "    samples, rewards, accepts = rlmh.train(subkey, num_iterations=num_train)\n",
    "    \n",
    "    print(f\"Overall acceptance rate: {np.mean(accepts):.3f}\")\n",
    "    \n",
    "    return rlmh, samples, rewards, accepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AMH warm-up...\n",
      "Warm-up acceptance rate: 0.411\n",
      "Final lambda: 9.24790\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret value of type <class '__main__.RLMH'> as an abstract array; it does not have a dtype attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mlogaddexp(log_p1, log_p2)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Run RLMH experiment\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m rlmh, samples, rewards, accepts \u001b[38;5;241m=\u001b[39m \u001b[43mrun_rlmh_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_log_density\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     21\u001b[0m plot_results(rlmh\u001b[38;5;241m.\u001b[39mwarmup(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m1\u001b[39m), m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m), samples, target_log_density)\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mrun_rlmh_experiment\u001b[0;34m(target_log_density, d, num_warmup, num_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Train RLMH\u001b[39;00m\n\u001b[1;32m     37\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m---> 38\u001b[0m samples, rewards, accepts \u001b[38;5;241m=\u001b[39m \u001b[43mrlmh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall acceptance rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accepts)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rlmh, samples, rewards, accepts\n",
      "Cell \u001b[0;32mIn[16], line 342\u001b[0m, in \u001b[0;36mRLMH.train\u001b[0;34m(self, key, num_iterations, update_every, log_every)\u001b[0m\n\u001b[1;32m    338\u001b[0m accepts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_iterations, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Take a step\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     key, reward, accept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     samples[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_state[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md]\n",
      "Cell \u001b[0;32mIn[16], line 203\u001b[0m, in \u001b[0;36mRLMH.step\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    200\u001b[0m phi_prop \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md:]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Compute acceptance probability\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m log_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_acceptance_probability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_prop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphi_prop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Compute reward\u001b[39;00m\n\u001b[1;32m    206\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_reward(x, x_prop, log_alpha)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/jax/_src/api_util.py:577\u001b[0m, in \u001b[0;36m_shaped_abstractify_slow\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    575\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(x\u001b[38;5;241m.\u001b[39mdtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot interpret value of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as an abstract array; it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not have a dtype attribute\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m core\u001b[38;5;241m.\u001b[39mShapedArray(np\u001b[38;5;241m.\u001b[39mshape(x), dtype, weak_type\u001b[38;5;241m=\u001b[39mweak_type,\n\u001b[1;32m    581\u001b[0m                         named_shape\u001b[38;5;241m=\u001b[39mnamed_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret value of type <class '__main__.RLMH'> as an abstract array; it does not have a dtype attribute"
     ]
    }
   ],
   "source": [
    "# Define target density (e.g., Gaussian mixture)\n",
    "def target_log_density(x):\n",
    "    # 2-component Gaussian mixture\n",
    "    mu1 = jnp.array([-5.0])\n",
    "    mu2 = jnp.array([5.0])\n",
    "    sigma = 1.0\n",
    "    \n",
    "    log_p1 = -0.5 * jnp.sum(((x - mu1) / sigma) ** 2)\n",
    "    log_p2 = -0.5 * jnp.sum(((x - mu2) / sigma) ** 2)\n",
    "    return jnp.logaddexp(log_p1, log_p2)\n",
    "\n",
    "# Run RLMH experiment\n",
    "rlmh, samples, rewards, accepts = run_rlmh_experiment(\n",
    "    target_log_density, \n",
    "    d=1,\n",
    "    num_warmup=5000,\n",
    "    num_train=20000\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "plot_results(rlmh.warmup(jax.random.PRNGKey(1), m=5000), samples, target_log_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
