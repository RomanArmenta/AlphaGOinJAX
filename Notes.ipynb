{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning applied to physical problems\n",
    "\n",
    "## Introduction to RL\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**. The agent takes **actions** to change the **state** of the environment and recieves **rewards** (or penalties) as feedback. The goal of the agent is to learn a **policy** --a strategy for choosing actions-- that maximizes the cumulative reward over time.\n",
    "\n",
    "<img src=\"img/agentenv.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "### Key concepts in RL\n",
    "\n",
    "Here, we summarize some of the key concepts in Reinforcement Learning in order to understand as good as possible what problems can RL solve, and far more.\n",
    "\n",
    "1. **Environment**\n",
    "    - The **environment** is the *worlds* in which the agent operates.\n",
    "    - The environment provides feedbackto the agent based on its actions.\n",
    "\n",
    "2. **State** (s)\n",
    "    - A **state** represents the current situation of the environment.\n",
    "    - The state changes as the agents take actions.\n",
    "\n",
    "3. **Action** (a)\n",
    "    - An **action** is a decision made by the agent that affects the environment. \n",
    "    - The set of all possible actions is called the *action space*.\n",
    "\n",
    "4. **Reward** (r)\n",
    "    - A **reward** $R_t$ is a feedback from the environment that evaluates the agent's action at time $t$.\n",
    "    - The agent's goal is to maximize the cumulative reward over time.\n",
    "    - The **expected return** $G_t$ is the cummulative reward, some specific function of the reward sequence after time $t$.\n",
    "\n",
    "5. **Policy** ($\\pi$)\n",
    "    - The *policy* is the strategy the agent uses to choose actions based on the current state. It maps states to actions.\n",
    "    - It can be written as $\\pi(a|s)$, which means *the probability of choosing the action* $a$ *given that the environment is in state* $s$.\n",
    "\n",
    "6. **Value Function** (v)\n",
    "    - The *value function* estimates the expected cumulative reward from a given state, assuming the agent follows its policy.\n",
    "    - It can be written as: $v_\\pi (s) = \\mathbb{E}_\\pi [G_t | S_t = s]$. That is: the expected value (or mean value) of the cummulative reward after time $t$, given that, at that time $t$ \n",
    "\n",
    "7. **Episode**\n",
    "    - An *episode* is a sequence of states, actions and rewards that ends in a terminal state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
